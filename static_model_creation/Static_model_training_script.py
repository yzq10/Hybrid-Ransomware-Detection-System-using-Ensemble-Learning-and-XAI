"""
Final Ensemble Training Script for Static Ransomware Detection
Trains XGBoost + SVM + Random Forest on combined PE + API/DLL features
Generates models and SVM preprocessor for production deployment
"""

import pandas as pd
import numpy as np
import pickle
import logging
import time
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score, 
    precision_score, recall_score, f1_score, roc_auc_score,
    roc_curve, precision_recall_curve
)

# ML Models
import xgboost as xgb
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class FinalEnsembleTrainer:
    """Train and evaluate the final ensemble for static ransomware detection."""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.svm_scaler = None
        self.feature_names = None
        self.results = {}
        
    def load_data(self, csv_path):
        """Load the final combined static features dataset."""
        logging.info("="*60)
        logging.info("LOADING FINAL STATIC FEATURES DATASET")
        logging.info("="*60)
        
        # Load dataset
        logging.info(f"Loading dataset from: {csv_path}")
        df = pd.read_csv(csv_path)
        logging.info(f"Dataset shape: {df.shape}")
        
        # Identify columns
        identifier_cols = ['SHA256', 'filename']
        family_cols = [col for col in df.columns if col.startswith('_family')]
        target_col = 'Malware_Type'
        
        # Feature columns (PE_ and API_ prefixed)
        pe_feature_cols = [col for col in df.columns if col.startswith('PE_')]
        api_feature_cols = [col for col in df.columns if col.startswith('API_')]
        feature_cols = pe_feature_cols + api_feature_cols
        
        logging.info(f"Dataset Analysis:")
        logging.info(f"  - Total samples: {len(df):,}")
        logging.info(f"  - PE header features: {len(pe_feature_cols)}")
        logging.info(f"  - API/DLL features: {len(api_feature_cols)}")
        logging.info(f"  - Total features: {len(feature_cols)}")
        logging.info(f"  - Identifier columns: {len(identifier_cols)}")
        logging.info(f"  - Family columns: {len(family_cols)}")
        
        # Check class distribution
        class_dist = df[target_col].value_counts().sort_index()
        logging.info(f"Class Distribution:")
        logging.info(f"  - Benign (0): {class_dist.get(0, 0):,} ({class_dist.get(0, 0)/len(df)*100:.1f}%)")
        logging.info(f"  - Ransomware (1): {class_dist.get(1, 0):,} ({class_dist.get(1, 0)/len(df)*100:.1f}%)")
        
        # Check for missing values
        missing_values = df[feature_cols].isnull().sum().sum()
        if missing_values > 0:
            logging.warning(f"Found {missing_values} missing values in features")
            df[feature_cols] = df[feature_cols].fillna(0)
        else:
            logging.info("‚úÖ No missing values found")
        
        # Prepare features and target
        X = df[feature_cols]
        y = df[target_col]
        
        # Store feature names for later use
        self.feature_names = feature_cols
        
        logging.info(f"‚úÖ Data loaded successfully")
        logging.info(f"   Features shape: {X.shape}")
        logging.info(f"   Target shape: {y.shape}")
        
        return X, y, df
    
    def create_models(self):
        """Create the three ensemble models with optimized hyperparameters."""
        logging.info("\n" + "="*60)
        logging.info("CREATING ENSEMBLE MODELS")
        logging.info("="*60)
        
        # XGBoost - Excellent for mixed numerical/binary features
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            min_child_weight=3,
            gamma=0.1,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=self.random_state,
            eval_metric='logloss',
            verbosity=0,
            n_jobs=-1
        )
        
        # SVM - Distance-based with RBF kernel
        svm_model = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            class_weight='balanced',
            random_state=self.random_state,
            probability=True,  # Enable probability predictions
            cache_size=1000    # Increase cache for faster training
        )
        
        # Random Forest - Robust tree ensemble
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=4,
            max_features='sqrt',
            class_weight='balanced',
            random_state=self.random_state,
            n_jobs=-1
        )
        
        self.models = {
            'XGBoost': xgb_model,
            'SVM': svm_model,
            'RandomForest': rf_model
        }
        
        logging.info("‚úÖ Models created:")
        for name, model in self.models.items():
            logging.info(f"   - {name}: {type(model).__name__}")
        
        return self.models
    
    def train_individual_models(self, X_train, X_test, y_train, y_test):
        """Train each model individually with proper preprocessing."""
        logging.info("\n" + "="*60)
        logging.info("TRAINING INDIVIDUAL MODELS")
        logging.info("="*60)
        
        individual_results = {}
        
        for model_name, model in self.models.items():
            logging.info(f"\nTraining {model_name}...")
            start_time = time.time()
            
            try:
                if model_name == 'SVM':
                    # SVM requires preprocessing
                    logging.info("  Applying StandardScaler for SVM...")
                    self.svm_scaler = StandardScaler()
                    X_train_scaled = self.svm_scaler.fit_transform(X_train)
                    X_test_scaled = self.svm_scaler.transform(X_test)
                    
                    # Train SVM on scaled features
                    model.fit(X_train_scaled, y_train)
                    
                    # Predictions on scaled features
                    y_pred = model.predict(X_test_scaled)
                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
                    
                else:
                    # XGBoost and Random Forest use raw features
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # Calculate metrics
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred, zero_division=0),
                    'recall': recall_score(y_test, y_pred, zero_division=0),
                    'f1': f1_score(y_test, y_pred, zero_division=0),
                    'auc': roc_auc_score(y_test, y_pred_proba),
                    'training_time': time.time() - start_time
                }
                
                individual_results[model_name] = {
                    'model': model,
                    'metrics': metrics,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba
                }
                
                logging.info(f"  ‚úÖ {model_name} trained successfully")
                logging.info(f"     Accuracy: {metrics['accuracy']:.4f}")
                logging.info(f"     F1-Score: {metrics['f1']:.4f}")
                logging.info(f"     AUC: {metrics['auc']:.4f}")
                logging.info(f"     Time: {metrics['training_time']:.1f}s")
                
            except Exception as e:
                logging.error(f"  ‚ùå Error training {model_name}: {str(e)}")
                individual_results[model_name] = {
                    'model': None,
                    'metrics': None,
                    'error': str(e)
                }
        
        return individual_results
    
    def evaluate_ensemble(self, individual_results, y_test):
        """Evaluate ensemble performance using majority voting."""
        logging.info("\n" + "="*60)
        logging.info("EVALUATING ENSEMBLE PERFORMANCE")
        logging.info("="*60)
        
        # Get predictions from successful models
        model_predictions = {}
        model_probabilities = {}
        
        for model_name, result in individual_results.items():
            if result.get('predictions') is not None:
                model_predictions[model_name] = result['predictions']
                model_probabilities[model_name] = result['probabilities']
        
        if len(model_predictions) < 2:
            logging.error("‚ùå Need at least 2 successful models for ensemble")
            return None
        
        logging.info(f"Ensemble composition: {list(model_predictions.keys())}")
        
        # Majority voting
        ensemble_predictions = []
        ensemble_probabilities = []
        
        for i in range(len(y_test)):
            # Get votes from each model
            votes = [pred[i] for pred in model_predictions.values()]
            probs = [prob[i] for prob in model_probabilities.values()]
            
            # Majority vote (if tie, use probability average)
            majority_vote = int(sum(votes) >= len(votes) / 2)
            avg_probability = np.mean(probs)
            
            ensemble_predictions.append(majority_vote)
            ensemble_probabilities.append(avg_probability)
        
        # Calculate ensemble metrics
        ensemble_metrics = {
            'accuracy': accuracy_score(y_test, ensemble_predictions),
            'precision': precision_score(y_test, ensemble_predictions, zero_division=0),
            'recall': recall_score(y_test, ensemble_predictions, zero_division=0),
            'f1': f1_score(y_test, ensemble_predictions, zero_division=0),
            'auc': roc_auc_score(y_test, ensemble_probabilities)
        }
        
        # Voting analysis
        voting_analysis = self.analyze_voting(model_predictions, ensemble_predictions, y_test)
        
        ensemble_results = {
            'metrics': ensemble_metrics,
            'predictions': ensemble_predictions,
            'probabilities': ensemble_probabilities,
            'voting_analysis': voting_analysis,
            'individual_results': individual_results
        }
        
        # Log results
        self.log_performance_table(individual_results, ensemble_metrics)
        
        return ensemble_results
    
    def analyze_voting(self, model_predictions, ensemble_predictions, y_test):
        """Analyze voting patterns and disagreements."""
        voting_stats = {
            'unanimous_correct': 0,
            'unanimous_incorrect': 0,
            'majority_correct': 0,
            'majority_incorrect': 0,
            'disagreements': []
        }
        
        model_names = list(model_predictions.keys())
        
        for i in range(len(y_test)):
            votes = [model_predictions[model][i] for model in model_names]
            ensemble_pred = ensemble_predictions[i]
            true_label = y_test.iloc[i]
            
            # Check if unanimous
            if len(set(votes)) == 1:  # All models agree
                if ensemble_pred == true_label:
                    voting_stats['unanimous_correct'] += 1
                else:
                    voting_stats['unanimous_incorrect'] += 1
            else:  # Models disagree
                if ensemble_pred == true_label:
                    voting_stats['majority_correct'] += 1
                else:
                    voting_stats['majority_incorrect'] += 1
                
                # Record disagreement
                disagreement = {
                    'index': i,
                    'true_label': true_label,
                    'ensemble_prediction': ensemble_pred,
                    'votes': dict(zip(model_names, votes))
                }
                voting_stats['disagreements'].append(disagreement)
        
        return voting_stats
    
    def plot_individual_confusion_matrices(self, cv_stats, output_dir="models/final_ensemble"):
        """Create individual confusion matrix plots for each model using CV aggregate data."""
        logging.info("\n" + "="*60)
        logging.info("GENERATING INDIVIDUAL CONFUSION MATRIX PLOTS")
        logging.info("="*60)
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        models = ['XGBoost', 'SVM', 'RandomForest', 'Ensemble']
        plot_files = []
        
        for model in models:
            if model in cv_stats:
                # Create individual figure
                plt.figure(figsize=(8, 6))
                
                # Calculate aggregate confusion matrix from CV stats
                precision = cv_stats[model]['precision']['mean']
                recall = cv_stats[model]['recall']['mean']
                accuracy = cv_stats[model]['accuracy']['mean']
                
                # Estimate confusion matrix values (for ~432 test samples per fold average)
                total_samples = 432
                positive_samples = int(total_samples * 0.474)  # ~47.4% ransomware
                negative_samples = total_samples - positive_samples
                
                # Calculate estimates
                tp = int(positive_samples * recall)
                fn = positive_samples - tp
                
                if precision > 0:
                    fp = int(tp * (1 - precision) / precision)
                else:
                    fp = 0
                tn = negative_samples - fp
                
                # Create confusion matrix
                cm = np.array([[tn, fp], [fn, tp]])
                
                # Plot heatmap
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                        xticklabels=['Benign', 'Ransomware'],
                        yticklabels=['Benign', 'Ransomware'],
                        square=True, cbar_kws={'shrink': 0.8},
                        annot_kws={'size': 18})
                
                # Add title with performance metrics
                plt.title(f'{model} - Confusion Matrix',
                        fontsize=22, fontweight='bold', pad=20)
                
                plt.xlabel('Predicted Label', fontsize=20)
                plt.ylabel('True Label', fontsize=20)

                # Make tick labels bigger
                plt.xticks(fontsize=16)  # ‚Üê X-axis tick labels (Benign/Ransomware)
                plt.yticks(fontsize=16)  # ‚Üê Y-axis tick labels (Benign/Ransomware)
                
                # Special formatting for ensemble
                if model == 'Ensemble':
                    plt.gca().spines['top'].set_color('red')
                    plt.gca().spines['right'].set_color('red')
                    plt.gca().spines['bottom'].set_color('red')
                    plt.gca().spines['left'].set_color('red')
                    plt.gca().spines['top'].set_linewidth(3)
                    plt.gca().spines['right'].set_linewidth(3)
                    plt.gca().spines['bottom'].set_linewidth(3)
                    plt.gca().spines['left'].set_linewidth(3)
                
                plt.tight_layout()
                
                # Save individual plot
                plot_file = output_path / f"confusion_matrix_{model.lower()}_cv.png"
                plt.savefig(plot_file, dpi=300, bbox_inches='tight')
                plt.show()
                
                plot_files.append(str(plot_file))
                logging.info(f"‚úÖ Saved {model} confusion matrix: {plot_file}")
                
                # Close figure to free memory
                plt.close()
            
            else:
                logging.warning(f"‚ö†Ô∏è No CV data available for {model}")
        
        return plot_files


    def cross_validate_ensemble(self, X, y, cv_folds=5):
        """Perform cross-validation on the ensemble."""
        logging.info("\n" + "="*60)
        logging.info("CROSS-VALIDATION EVALUATION")
        logging.info("="*60)
        
        cv_splitter = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        # Initialize results dictionary for all metrics
        cv_results = {}
        for model_name in self.models.keys():
            cv_results[model_name] = {
                'accuracy': [], 'precision': [], 'recall': [], 'f1': []
            }
        cv_results['Ensemble'] = {
            'accuracy': [], 'precision': [], 'recall': [], 'f1': []
        }
        
        for fold, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y)):
            logging.info(f"Processing fold {fold + 1}/{cv_folds}...")
            
            X_train_cv = X.iloc[train_idx]
            X_val_cv = X.iloc[val_idx]
            y_train_cv = y.iloc[train_idx]
            y_val_cv = y.iloc[val_idx]
            
            # Train models for this fold
            fold_results = self.train_individual_models(X_train_cv, X_val_cv, y_train_cv, y_val_cv)
            
            # Store individual model results (all metrics)
            for model_name, result in fold_results.items():
                if result.get('metrics'):
                    metrics = result['metrics']
                    cv_results[model_name]['accuracy'].append(metrics['accuracy'])
                    cv_results[model_name]['precision'].append(metrics['precision'])
                    cv_results[model_name]['recall'].append(metrics['recall'])
                    cv_results[model_name]['f1'].append(metrics['f1'])
                else:
                    cv_results[model_name]['accuracy'].append(0.0)
                    cv_results[model_name]['precision'].append(0.0)
                    cv_results[model_name]['recall'].append(0.0)
                    cv_results[model_name]['f1'].append(0.0)
            
            # Calculate ensemble metrics for this fold
            if len([r for r in fold_results.values() if r.get('predictions') is not None]) >= 2:
                model_predictions = {name: result['predictions'] for name, result in fold_results.items() 
                                    if result.get('predictions') is not None}
                
                # Simple majority voting
                ensemble_predictions = []
                for i in range(len(y_val_cv)):
                    votes = [pred[i] for pred in model_predictions.values()]
                    majority_vote = int(sum(votes) >= len(votes) / 2)
                    ensemble_predictions.append(majority_vote)
                
                # Calculate all ensemble metrics
                ensemble_accuracy = accuracy_score(y_val_cv, ensemble_predictions)
                ensemble_precision = precision_score(y_val_cv, ensemble_predictions, zero_division=0)
                ensemble_recall = recall_score(y_val_cv, ensemble_predictions, zero_division=0)
                ensemble_f1 = f1_score(y_val_cv, ensemble_predictions, zero_division=0)
                
                cv_results['Ensemble']['accuracy'].append(ensemble_accuracy)
                cv_results['Ensemble']['precision'].append(ensemble_precision)
                cv_results['Ensemble']['recall'].append(ensemble_recall)
                cv_results['Ensemble']['f1'].append(ensemble_f1)
            else:
                cv_results['Ensemble']['accuracy'].append(0.0)
                cv_results['Ensemble']['precision'].append(0.0)
                cv_results['Ensemble']['recall'].append(0.0)
                cv_results['Ensemble']['f1'].append(0.0)
        
        # Calculate CV statistics for all metrics
        cv_stats = {}
        for model_name in cv_results.keys():
            cv_stats[model_name] = {}
            for metric in ['accuracy', 'precision', 'recall', 'f1']:
                scores = cv_results[model_name][metric]
                cv_stats[model_name][metric] = {
                    'mean': np.mean(scores),
                    'std': np.std(scores),
                    'scores': scores
                }
        
        # Log CV results for all metrics
        metrics_display = ['accuracy', 'precision', 'recall', 'f1']
        for metric in metrics_display:
            logging.info(f"\nCross-Validation Results ({metric.upper()}):")
            for model_name in ['XGBoost', 'SVM', 'RandomForest', 'Ensemble']:
                if model_name in cv_stats:
                    stats = cv_stats[model_name][metric]
                    logging.info(f"  {model_name:<12}: {stats['mean']:.4f} (+/- {stats['std']*2:.4f})")
        
        return cv_stats
    
    def save_models(self, output_dir="models/final_ensemble"):
        """Save trained models and preprocessors."""
        logging.info("\n" + "="*60)
        logging.info("SAVING TRAINED MODELS")
        logging.info("="*60)
        
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        # Save individual models
        for model_name, model in self.models.items():
            if model is not None:
                model_file = output_path / f"{model_name.lower()}_model.pkl"
                with open(model_file, 'wb') as f:
                    pickle.dump(model, f)
                saved_files.append(str(model_file))
                logging.info(f"‚úÖ Saved {model_name} model: {model_file}")
        
        # Save SVM scaler
        if self.svm_scaler is not None:
            scaler_file = output_path / "svm_scaler.pkl"
            with open(scaler_file, 'wb') as f:
                pickle.dump(self.svm_scaler, f)
            saved_files.append(str(scaler_file))
            logging.info(f"‚úÖ Saved SVM scaler: {scaler_file}")
        
        # Save feature names as both PKL and JSON
        if self.feature_names is not None:
            # Save as pickle for backward compatibility
            features_pkl_file = output_path / "feature_names.pkl"
            with open(features_pkl_file, 'wb') as f:
                pickle.dump(self.feature_names, f)
            saved_files.append(str(features_pkl_file))
            logging.info(f"‚úÖ Saved feature names (PKL): {features_pkl_file}")
            
            # Save as JSON for easy reading/debugging
            features_json_file = output_path / "feature_names.json"
            feature_data = {
                'feature_names': self.feature_names,
                'feature_count': len(self.feature_names),
                'pe_features': [f for f in self.feature_names if f.startswith('PE_')],
                'api_features': [f for f in self.feature_names if f.startswith('API_')],
                'dll_features': [f for f in self.feature_names if f.startswith('DLL_')],
                'pe_count': len([f for f in self.feature_names if f.startswith('PE_')]),
                'api_count': len([f for f in self.feature_names if f.startswith('API_')]),
                'dll_count': len([f for f in self.feature_names if f.startswith('DLL_')])
            }
            
            import json
            with open(features_json_file, 'w') as f:
                json.dump(feature_data, f, indent=2)
            saved_files.append(str(features_json_file))
            logging.info(f"‚úÖ Saved feature names (JSON): {features_json_file}")
        
        # Save enhanced metadata with feature breakdown
        metadata = {
            'models': list(self.models.keys()),
            'feature_count': len(self.feature_names) if self.feature_names else 0,
            'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'random_state': self.random_state,
            'feature_breakdown': {
                'pe_features': len([f for f in self.feature_names if f.startswith('PE_')]) if self.feature_names else 0,
                'api_features': len([f for f in self.feature_names if f.startswith('API_')]) if self.feature_names else 0,
                'dll_features': len([f for f in self.feature_names if f.startswith('DLL_')]) if self.feature_names else 0
            },
            'model_info': {
                'xgboost': {
                    'type': 'XGBClassifier',
                    'preprocessing': 'none'
                },
                'svm': {
                    'type': 'SVC',
                    'preprocessing': 'StandardScaler'
                },
                'randomforest': {
                    'type': 'RandomForestClassifier', 
                    'preprocessing': 'none'
                }
            }
        }
        
        metadata_file = output_path / "ensemble_metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        saved_files.append(str(metadata_file))
        logging.info(f"‚úÖ Saved metadata: {metadata_file}")
        
        return saved_files
    
    def plot_results(self, ensemble_results, output_dir="models/final_ensemble"):
        """Create visualization plots for the ensemble results."""
        logging.info("\n" + "="*60)
        logging.info("GENERATING RESULT PLOTS")
        logging.info("="*60)
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Plot 1: Model Comparison
        plt.figure(figsize=(15, 10))
        
        # Subplot 1: Accuracy Comparison
        plt.subplot(2, 3, 1)
        models = list(ensemble_results['individual_results'].keys()) + ['Ensemble']
        accuracies = [ensemble_results['individual_results'][m]['metrics']['accuracy'] 
                     for m in ensemble_results['individual_results'].keys()]
        accuracies.append(ensemble_results['metrics']['accuracy'])
        
        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'orange', 'red'])
        plt.title('Model Accuracy Comparison')
        plt.ylabel('Accuracy')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        
        # Add value labels on bars
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{acc:.3f}', ha='center', va='bottom')
        
        # Subplot 2: F1-Score Comparison
        plt.subplot(2, 3, 2)
        f1_scores = [ensemble_results['individual_results'][m]['metrics']['f1'] 
                    for m in ensemble_results['individual_results'].keys()]
        f1_scores.append(ensemble_results['metrics']['f1'])
        
        bars = plt.bar(models, f1_scores, color=['skyblue', 'lightgreen', 'orange', 'red'])
        plt.title('Model F1-Score Comparison')
        plt.ylabel('F1-Score')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        
        for bar, f1 in zip(bars, f1_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{f1:.3f}', ha='center', va='bottom')
        
        # Subplot 3: AUC Comparison
        plt.subplot(2, 3, 3)
        aucs = [ensemble_results['individual_results'][m]['metrics']['auc'] 
               for m in ensemble_results['individual_results'].keys()]
        aucs.append(ensemble_results['metrics']['auc'])
        
        bars = plt.bar(models, aucs, color=['skyblue', 'lightgreen', 'orange', 'red'])
        plt.title('Model AUC Comparison')
        plt.ylabel('AUC')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        
        for bar, auc in zip(bars, aucs):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{auc:.3f}', ha='center', va='bottom')
        
        # Subplot 4: Voting Analysis
        plt.subplot(2, 3, 4)
        voting_stats = ensemble_results['voting_analysis']
        categories = ['Unanimous\nCorrect', 'Unanimous\nIncorrect', 
                     'Majority\nCorrect', 'Majority\nIncorrect']
        values = [voting_stats['unanimous_correct'], voting_stats['unanimous_incorrect'],
                 voting_stats['majority_correct'], voting_stats['majority_incorrect']]
        
        colors = ['green', 'red', 'lightgreen', 'pink']
        plt.bar(categories, values, color=colors)
        plt.title('Ensemble Voting Analysis')
        plt.ylabel('Number of Samples')
        plt.xticks(rotation=45)
        
        for i, v in enumerate(values):
            plt.text(i, v + 0.5, str(v), ha='center', va='bottom')
        
        # Subplot 5: Confusion Matrix
        plt.subplot(2, 3, 5)
        y_test = ensemble_results.get('y_test', [])  # You'll need to pass this
        if len(y_test) > 0:
            cm = confusion_matrix(y_test, ensemble_results['predictions'])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Benign', 'Ransomware'],
                       yticklabels=['Benign', 'Ransomware'])
            plt.title('Ensemble Confusion Matrix')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
        
        plt.tight_layout()
        plot_file = output_path / "ensemble_performance.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        logging.info(f"‚úÖ Performance plots saved: {plot_file}")
        
        return str(plot_file)
    
    def train_final_ensemble(self, csv_path, output_dir="models/final_ensemble", 
                           test_size=0.2, run_cv=True):
        """Main training pipeline for the final ensemble."""
        logging.info("üöÄ STARTING FINAL ENSEMBLE TRAINING PIPELINE")
        logging.info("="*60)
        
        try:
            # Load data
            X, y, df = self.load_data(csv_path)
            
            # Create models
            self.create_models()
            
            # Train-test split
            logging.info(f"\nSplitting data: {100-test_size*100:.0f}% train, {test_size*100:.0f}% test")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=self.random_state, 
                stratify=y
            )
            
            logging.info(f"Training set: {X_train.shape[0]:,} samples")
            logging.info(f"Test set: {X_test.shape[0]:,} samples")
            
            # Train individual models
            individual_results = self.train_individual_models(X_train, X_test, y_train, y_test)
            
            # Evaluate ensemble
            ensemble_results = self.evaluate_ensemble(individual_results, y_test)
            ensemble_results['y_test'] = y_test  # Add for plotting
            
            # Cross-validation (optional)
            if run_cv:
                cv_stats = self.cross_validate_ensemble(X, y)
                ensemble_results['cv_stats'] = cv_stats
            
            # Save models
            saved_files = self.save_models(output_dir)
            
            # Generate plots
            plot_file = self.plot_results(ensemble_results, output_dir)
            
            # Generate confusion matrix plots using CV data
            if run_cv and 'cv_stats' in ensemble_results:
                cm_plot_files = self.plot_individual_confusion_matrices(ensemble_results['cv_stats'], output_dir)

            # Final summary
            logging.info("\n" + "üéØ ENSEMBLE TRAINING COMPLETED SUCCESSFULLY! üéØ")
            logging.info("="*60)
            logging.info(f"‚úÖ Final Ensemble Performance:")
            logging.info(f"   Accuracy:  {ensemble_results['metrics']['accuracy']:.4f}")
            logging.info(f"   Precision: {ensemble_results['metrics']['precision']:.4f}")
            logging.info(f"   Recall:    {ensemble_results['metrics']['recall']:.4f}")
            logging.info(f"   F1-Score:  {ensemble_results['metrics']['f1']:.4f}")
            logging.info(f"   AUC:       {ensemble_results['metrics']['auc']:.4f}")
            
            logging.info(f"\n‚úÖ Models saved to: {output_dir}")
            logging.info(f"   - XGBoost model")
            logging.info(f"   - SVM model + StandardScaler")
            logging.info(f"   - Random Forest model")
            logging.info(f"   - Feature names & metadata")
            
            logging.info(f"\n‚úÖ Ready for production deployment!")
            logging.info(f"   Load models using: EnhancedStaticDetector.load_models('{output_dir}')")
            
            return ensemble_results
            
        except Exception as e:
            logging.error(f"‚ùå Training failed: {str(e)}")
            raise

    def log_performance_table(self, individual_results, ensemble_metrics):
        """Create and log a formatted performance table."""
        logging.info("\n" + "="*80)
        logging.info("PERFORMANCE SUMMARY TABLE")
        logging.info("="*80)
        
        # Table header
        header = f"{'Model':<15} {'Accuracy':<10} {'Precision':<11} {'Recall':<10} {'F1-Score':<10} {'AUC':<10}"
        logging.info(header)
        logging.info("-" * 80)
        
        # Individual model rows
        for model_name, result in individual_results.items():
            if result.get('metrics'):
                metrics = result['metrics']
                row = (f"{model_name:<15} "
                    f"{metrics['accuracy']:<10.4f} "
                    f"{metrics['precision']:<11.4f} "
                    f"{metrics['recall']:<10.4f} "
                    f"{metrics['f1']:<10.4f} "
                    f"{metrics['auc']:<10.4f}")
                logging.info(row)
        
        # Ensemble row
        row = (f"{'Ensemble':<15} "
            f"{ensemble_metrics['accuracy']:<10.4f} "
            f"{ensemble_metrics['precision']:<11.4f} "
            f"{ensemble_metrics['recall']:<10.4f} "
            f"{ensemble_metrics['f1']:<10.4f} "
            f"{ensemble_metrics['auc']:<10.4f}")
        logging.info(row)
        logging.info("="*80)

def main():
    """Main function to run the ensemble training pipeline."""
    
    # Configuration
    config = {
        'input_csv': "final_static_features_dataset.csv",
        'output_dir': "models/temp/static_ensemble",
        'test_size': 0.2,
        'random_state': 42,
        'run_cross_validation': True
    }
    
    # Verify input file exists
    if not Path(config['input_csv']).exists():
        logging.error(f"‚ùå Input dataset not found: {config['input_csv']}")
        logging.error("Please ensure you have run the dataset combination script first.")
        return
    
    # Initialize trainer
    trainer = FinalEnsembleTrainer(random_state=config['random_state'])
    
    # Run training pipeline
    results = trainer.train_final_ensemble(
        csv_path=config['input_csv'],
        output_dir=config['output_dir'],
        test_size=config['test_size'],
        run_cv=config['run_cross_validation']
    )
    
    return results


if __name__ == "__main__":
    main()